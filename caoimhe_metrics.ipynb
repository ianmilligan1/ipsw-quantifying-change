{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "import csv\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/caoimherooney/Desktop/227-www-ndp-ca.txt\n"
     ]
    }
   ],
   "source": [
    "# import NDP file (~3GB of plain text)\n",
    "ndp_file = \"/Users/caoimherooney/Desktop/227-www-ndp-ca.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the NDP plain text as a dataframe\n",
    "df = pd.read_csv(ndp_file, sep = \",\", usecols=[0,1,2,3], header=None, error_bad_lines=False, quoting=csv.QUOTE_NONE)\n",
    "df = df.sort_values(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is too much data in this dataframe!\n",
    "# let's focus just on the homepage\n",
    "# find a homepage to focus on for the diffs\n",
    "homepages = df.loc[df[2] == \"http://www.ndp.ca/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many homepages are there?\n",
    "len(homepages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(20051007',\n",
       " 'www.ndp.ca',\n",
       " 'http://www.ndp.ca/',\n",
       " \"NDP | The New Democratic Party of Canada ABOUT ›\\xa0Jack Layton ›\\xa0Our Caucus ›\\xa0Our History ›\\xa0Contact Us GET INVOLVED ›\\xa0Your Riding ›\\xa0Campaigns ›\\xa0Events ›\\xa0Youth RESOURCES ›\\xa0Press Room ›\\xa0Speeches & Articles ›\\xa0Downloads ›\\xa0e.NDP ›\\xa0More... SEARCH ›\\xa0Français\\xa0 › NDP Budget Details › Jack's Budget Speech › Quotes on NDP Budget › Session in Review Privacy Policy |\\xa0Jobs |\\xa0RSS © 2005 New Democratic Party\"]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at the first ROW of the dataframe\n",
    "# this should be, given sorting, the first crawl\n",
    "\n",
    "homepages.iloc[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's just strip the text out\n",
    "# this is the fourth column in the dataframe\n",
    "# (which is 3 in Python's start from 0 numbering)\n",
    "\n",
    "alltext = homepages[3].tolist()\n",
    "\n",
    "# alltext now contains all of the homepage text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's begin by comparing texts\n",
    "# we can load the SequenceMatcher library\n",
    "\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# byte-wise metric\n",
    "# compares two pages character by character\n",
    "# returns 1 if there is any change at all between two pages\n",
    "\n",
    "output = []\n",
    "for y in range (5,10):\n",
    "    first_page = \"\".join(homepages.iloc[y].tolist()) # convert webpage text into one long string\n",
    "    second_page = \"\".join(homepages.iloc[y+1].tolist()) \n",
    "    if len(first_page) == len(second_page):\n",
    "        for k in range(len(first_page)):\n",
    "            if first_page[k] != second_page[k]:\n",
    "                output.append(1)\n",
    "                break   \n",
    "            elif (k == len(first_page) - 1) and (first_page[k] == second_page[k]):\n",
    "                output.append(0)\n",
    "    else:\n",
    "        output.append(1)\n",
    "\n",
    "# output is the list of metric values (either 0 or 1)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "['cahttp', 'Contact', 'More', 'Contact', 'NDP', 'Us', 'Campaigns', 'Budget', 'Budget', 'Budget']\n",
      "6\n",
      "['Report', 'Our', 'New', 'ndp', 'Downloads', 'cahttp', 's', 'ca', 'own', '7']\n",
      "7\n",
      "['Events', 's', 'words', 'Report', 's', 'Caucus', 'Gomery', 'of', 'New', 'NDP']\n",
      "8\n",
      "['Jobs', 'ndp', 'Your', 'Want', 'www', 'Privacy', 'own', 'of', 'Party', 'Gomery']\n",
      "9\n",
      "['Youth', 'Feb', 'Party', 'O', 'Feb', 'continued', 'Wed', 'Thu', 'Newsletter', 'NDP']\n",
      "[0.3360599977930143, 0.0, 2.220446049250313e-16, 0.3377338214674782, 0.44381348960673783]\n"
     ]
    }
   ],
   "source": [
    "#print(type(first_page))\n",
    "#print(first_page)\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "output = []\n",
    "for y in range(5,10):\n",
    "    print(y)\n",
    "    first_page = re.sub(\"[^\\w]\", \" \",  \"\".join(homepages.iloc[y].tolist())).split()\n",
    "    second_page = re.sub(\"[^\\w]\", \" \",  \"\".join(homepages.iloc[y+1].tolist())).split()\n",
    "\n",
    "    # generate random list of words that appear in first page\n",
    "    # ** constrain length of words **\n",
    "    random_words = []\n",
    "    for i in range(10):\n",
    "        random_words.append(random.choice(first_page))\n",
    "    print(random_words)\n",
    "    \n",
    "    # count how many times random words appear in each page\n",
    "    v1 = []\n",
    "    v2 = []\n",
    "    for j in range(len(random_words)):\n",
    "        v1.append(first_page.count(random_words[j]))\n",
    "        v2.append(second_page.count(random_words[j]))\n",
    "        \n",
    "    D = 1 - np.inner(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "    output.append(D)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20051027www', 'ndp', 'cahttp', 'www', 'ndp', 'ca', 'NDP', 'The', 'New', 'Democratic', 'Party', 'of', 'Canada', 'ABOUT', 'Jack', 'Layton', 'Our', 'Caucus', 'Our', 'History', 'Contact', 'Us', 'GET', 'INVOLVED', 'Your', 'Riding', 'Campaigns', 'Events', 'Youth', 'RESOURCES', 'Press', 'Room', 'Speeches', 'Articles', 'Downloads', 'e', 'NDP', 'More', 'SEARCH', 'Français', 'NDP', 'Budget', 'Details', 'Jack', 's', 'Budget', 'Speech', 'Quotes', 'on', 'NDP', 'Budget', 'Session', 'in', 'Review', 'Privacy', 'Policy', 'Jobs', 'RSS', '2005', 'New', 'Democratic', 'Party']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "mystr = 'This is a string, with words!'\n",
    "mystr = re.sub(\"[^\\w]\", \" \", \"\".join(homepages.iloc[y].tolist())).split()\n",
    "print(mystr)\n",
    "#wordList = re.sub(\"[^\\w]\", \" \",  mystr).split()\n",
    "\n",
    "#print(wordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
