{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "import csv\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import NDP file (~3GB of plain text)\n",
    "ndp_file = \"227-www-ndp-ca.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the NDP plain text as a dataframe\n",
    "df = pd.read_csv(ndp_file, sep = \",\", usecols=[0,1,2,3], header=None, error_bad_lines=False, quoting=csv.QUOTE_NONE)\n",
    "df = df.sort_values(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n"
     ]
    }
   ],
   "source": [
    "# let's focus just on the homepage\n",
    "# find a homepage to focus on for the diffs\n",
    "homepages = df.loc[df[2].isin([\"http://www.ndp.ca/\",\"https://www.ndp.ca/\"])]\n",
    "print(len(homepages))\n",
    "homepage_text = homepages[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's begin by comparing texts\n",
    "# we can load the SequenceMatcher library\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "lists = [ [] for _ in range(5) ]\n",
    "labels = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = homepages[0].tolist()\n",
    "newdates = [x[1:] for x in dates]\n",
    "lists[0] = newdates\n",
    "labels.append('dates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# byte-wise metric\n",
    "# compares two pages character by character\n",
    "# returns 1 if there is any change at all between two pages\n",
    "\n",
    "bytewise = []\n",
    "for y in range(len(homepages)-1):\n",
    "    first_page = homepage_text.iloc[y]\n",
    "    second_page = homepage_text.iloc[y+1]\n",
    "    if len(first_page) == len(second_page):\n",
    "        for k in range(len(first_page)):\n",
    "            if first_page[k] != second_page[k]:\n",
    "                bytewise.append(1)\n",
    "                break   \n",
    "            elif (k == len(first_page) - 1) and (first_page[k] == second_page[k]):\n",
    "                bytewise.append(0)\n",
    "    else:\n",
    "        bytewise.append(1)\n",
    "\n",
    "# output is the list of metric values: either 0 (no change) or 1 (any change)\n",
    "# print(bytewise)\n",
    "lists[1] = bytewise\n",
    "labels.append('byte-wise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF.IDF metric\n",
    "# compares the occurrence of chosen words in each page\n",
    "# generates vectors weighted by such occurrences and calculates cosine distance between them\n",
    "\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "tfidf = []\n",
    "for y in range(len(homepages)-1):\n",
    "    \n",
    "    first_page = re.sub(\"[^\\w]\", \" \",  \"\".join(homepage_text.iloc[y])).split()\n",
    "    second_page = re.sub(\"[^\\w]\", \" \",  \"\".join(homepage_text.iloc[y+1])).split()\n",
    "\n",
    "    p1 = [x for x in first_page if re.match(\"\\w+\",x)]\n",
    "    p2 = [s.lower() for s in p1]\n",
    "    p3 = [word for word in p2 if word not in stop_words]\n",
    "    \n",
    "    first_page_unique = np.unique(np.asarray(p3))\n",
    "    \n",
    "    v1 = []\n",
    "    v2 = []\n",
    "     \n",
    "#     random_words = []\n",
    "#     for i in range(10):\n",
    "#         rand = random.choice(p3)\n",
    "#         print(rand)\n",
    "#         while len(rand) <= 4 or (rand in random_words):\n",
    "#             rand = random.choice(p3)    \n",
    "#         random_words.append(rand)\n",
    "\n",
    "#     for j in range(len(random_words)):\n",
    "#         v1.append(first_page.count(random_words[j]))\n",
    "#         v2.append(second_page.count(random_words[j]))\n",
    "        \n",
    "    for j in range(len(first_page_unique)):\n",
    "        v1.append(first_page.count(first_page_unique[j]))\n",
    "        v2.append(second_page.count(first_page_unique[j]))\n",
    "    \n",
    "    if np.linalg.norm(v2) == 0:\n",
    "        D = 1\n",
    "    else:\n",
    "        D = 1 - np.inner(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "    tfidf.append(D)\n",
    "\n",
    "# output is the list of metric values between 0 (very similar) and 1 (very different)\n",
    "# print(tfidf)\n",
    "lists[2] = tfidf\n",
    "labels.append('TF.IDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit distances metric\n",
    "# number of edits that are required to transform one sentence into another.\n",
    "# we normalise this number by the total number of characters in each of the two pages \n",
    "# ie deleting all of page 1 and adding all of page 2\n",
    "\n",
    "edit_distances = []\n",
    "for y in range(len(homepages)-1):\n",
    "    first_page = \"\".join(homepage_text.iloc[y]) # convert webpage text into one long string\n",
    "    second_page = \"\".join(homepage_text.iloc[y+1]) \n",
    "    len_first = len(first_page)\n",
    "    len_second = len(second_page)\n",
    "    \n",
    "    distance = nltk.edit_distance(first_page, second_page, transpositions=False)\n",
    "    edit_distances.append(distance / max(len_first,len_second))\n",
    "\n",
    "# output is the list of metric values between 0 (very similar) and 1 (very different)\n",
    "# print(edit_distances)\n",
    "lists[3] = edit_distances\n",
    "labels.append('edit distances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word distance metric \n",
    "# calculates how many of the words on a page have changed\n",
    "# split option allows us to consider if splitting the strings into words makes a difference\n",
    "\n",
    "word_distance = []\n",
    "word_distance_split = []\n",
    "for y in range (0,len(homepages)-1):\n",
    "    first_page = homepages.iloc[y]\n",
    "    second_page = homepages.iloc[y+1]\n",
    "    seq = SequenceMatcher(None, first_page[3],second_page[3])\n",
    "    seq_split = SequenceMatcher(None, first_page[3].split(),second_page[3].split())\n",
    "    distance = 1-seq.ratio()\n",
    "    distance_split = 1-seq_split.ratio()\n",
    "    word_distance.append(distance)\n",
    "    word_distance_split.append(distance_split)\n",
    "    \n",
    "# output is the list of metric values between 0 (very similar) and 1 (very different)\n",
    "# print(word_distance)\n",
    "# print(word_distance_split)\n",
    "lists[4] = word_distance\n",
    "#lists[5] = word_distance_split\n",
    "labels.append('word distances')\n",
    "#labels.append('word distances split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = homepages[0].unique()\n",
    "N = 10\n",
    "\n",
    "plt.figure(figsize = (15,5), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
    "for j in range(1, len(lists)):\n",
    "    my_xticks = []\n",
    "    for i in range (0,len(times)-1,N):\n",
    "        my_xticks.append(str(times[i][5:7] + \"/\" + times[i][3:5]));\n",
    "    plt.plot(lists[j], label = labels[j]);\n",
    "    plt.xticks(range(0,len(times)-1,N), my_xticks);\n",
    "    plt.legend(loc = \"upper left\", fontsize = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"text_metrics.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(list(map(list, zip(*lists))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trying to go one page deeper than homepage -- having problems with dataframes\n",
    "# find all links that begin with \"http://www.ndp.ca/\"\n",
    "subset_mask = df[2].str[:18] == \"http://www.ndp.ca/\"\n",
    "subset = df.loc[subset_mask]\n",
    "# df[2] = df[2].str[:18] # not sure which number\n",
    "# print(df[2].unique())\n",
    "\n",
    "# reduce data to those within 2 pages of homepage\n",
    "nextpage_mask = subset[2].str.len() > 18\n",
    "slash_mask = (subset[2].str.count('/') <= 4)\n",
    "subset = subset.loc[nextpage_mask]\n",
    "subset = subset.loc[slash_mask]\n",
    "\n",
    "# add slashes to data\n",
    "for i in range(len(subset[2])):\n",
    "    string = subset.loc[subset.index[i], 2]\n",
    "    print(string)\n",
    "    final_char = string[len(string)-1]\n",
    "    if final_char != '/':\n",
    "        subset[2][i] = string + '/'\n",
    "    else:\n",
    "        subset[2][i] = string\n",
    "\n",
    "slash_mask2 = (subset[2].str.count('/') == 4)\n",
    "subset = subset.loc[slash_mask2]\n",
    "print(subset)\n",
    "\n",
    "#lngth = []\n",
    "#for i in range(len(df[2])):\n",
    "#    locations = [pos for pos, char in enumerate(df[2][i]) if char == '/']\n",
    "#    lngth.append(len(locations))\n",
    "# print(lngth)\n",
    "\n",
    "#nextpages = df.loc[lngth == 3]\n",
    "\n",
    "#nextpages = df.loc[len(locations) == 4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
